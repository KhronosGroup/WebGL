<!DOCTYPE html>
<!-- Questions to answer

Q: Do videos with variable framerate have a target display rate?
A: No such thing is exposed in the HTMLVideoElement or MediaElement interfaces.
   The containers usually store the framerate as metadata (in WebM FrameRate
   is listed as "information only."). However browsers may not use that.
   Firefox 4, WebKit and Opera all reportedly use the presentation time
   encoded with the frame.
   See this thread:
   http://lists.whatwg.org/pipermail/whatwg-whatwg.org/2011-January/029801.html
   The thread starts at
   http://lists.whatwg.org/pipermail/whatwg-whatwg.org/2011-January/029724.html
   There is no resolution to the request for a way to determine fps.

   WebM container spec: http://www.webmproject.org/code/specs/container/ 

Q: What do applications need to be able to determine about the source?
A: Two things
   - the minimum interframe interval, which is probably the same as the
     framerate given in the container. This is to determine a rendering
	 budget. Need to ensure methodology works with fixed- and variable-
	 frame-rate videos.
   - whether a frame has been missed. In EGLStream this can be determined
     by comparing the PRODUCER_FRAME_KHR and CONSUMER_FRAME_KHR attributes.

Q: Should we make connection setup a convenience function with an onload
   handler callback?
A: TBD

Q: Use an exponential moving average for calculating latency?
A: TBD.

Q: What should be the interface for connecting a stream to a producer?
A: KHR_stream_producer_aldatalocator has the app pass an XADataSink
   structure with, essentially, a pointer to the EGLStream, as the
   ImageVideoSnk argument when calling CreateMediaPlayer.

   Similarly the stream is connected to an EGLSurface producer by
   eglCreateStreamProducerSurfaceKHR(dpy, config, EGLStreamKHR stream, &attrib_list[0]);

   This suggests the stream needs to be passed when creating the video element.
   But video elements do not have a constructor. 

Q: Is the maximum latency set when the producer is bound?
A: Yes.

Q: Do we specify repeat vs pause for the case where latency drops? If so, how?
A: TBD.

Q: How to ensure there is only 1 stream per producer?
A: TBD. Possibly make the stream constructor require the source object.

Q: What are the units of DOMHighResTimeStamp and the base for window.performance.now?
A: A double accurate to "a thousandth of a millisecond" but accuracy is allowed to be as sloppy as "to 1ms."
   The base is navigationStart, more or less when the browser begins fetching the document. Times are
   monotonically increasing.
   
Q: What are the units and base of the next frame attribute on a MediaElement?
A: There is no next frame attribute. There is a currentTime attribute. It is a double in units of seconds
   with 0 at the start of the media. In essence it is the MSC.
   
Q: How many seconds can be represented using a double (53 bits) with units of nanoseconds?
A: 9007199s = 150119m = 2501h = 41days enough for an MSC. Maybe not enough for UST (uptime).

Q: Using a double with units of 1s, starting with t0 = 2**32, what is the range and precision?
A: The exponent, and hence precision, will remain constant for the next ~4 billion seconds, or ~136 years.
   This precision is approximately one microsecond.
   
Q: What is the type and precision of the frame times in Matroska/WebM?
A: Base units is nanoseconds but header contains a TimecodeScale multiplier that SHOULD/MUST be set to
   the default 1,000,000, so it is in milliseconds. But you can't do 29.97fps acurately with milliseconds.
   Raw timecode = (a + b) * c where
   a = Block's Timecode
   b = Cluster's Timecode
   c = TimecodeScale
   a is a signed 16-bit integer. Cluster timecode is an unsigned int which can be up to 56 bits.
   XXX check other formats.
   
   Timestamps for VP8 carried in RTP are based on a sampling rate of 90kHz!!
-->
<!--
/*
 * WEBGL_dynamic_texture sample
 *
 * Copyright (C) 2012 HI Corporation. All Rights Reserved.
 *
 * Based on
 *   http://www.khronos.org/registry/webgl/sdk/demos/webkit/SpiritBox.html
 * which carries the following license which propagates to this sample:
 * 
 * Copyright (C) 2009 Apple Inc. All Rights Reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND HI CORPORATION ``AS IS''
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
 * OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
 -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>WEBGL_dynamic_texture Sample</title>
<style>
body, html {
  margin: 0px;
  width: 100%;
  height: 100%;
}
#framerate {
  position: absolute;
  top: 10px;
  left: 10px;
  background-color: rgba(0,0,0,0.3);
  padding: 1em;
  color: white;
}
#example {
  width: 100%;
  height: 100%;
}
canvas {
  display: block;
  border: thick ridge MediumSlateBlue;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
</style>

<!-- For {request,cancel}AnimFrame -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/common/webgl-utils.js"></script>
<!-- For lost context simulation -->
<script src="https://www.khronos.org/registry/webgl/sdk/debug/webgl-debug.js"></script>
<!-- For simpleSetup & makeBox -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DI.js"> </script>
<!-- For matrix math -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DIMath.js"> </script>

<script id="vshader" type="x-shader/x-vertex">
  uniform mat4 u_modelViewProjMatrix;
  uniform mat4 u_normalMatrix;
  uniform vec3 lightDir;

  attribute vec3 vNormal;
  attribute vec4 vTexCoord;
  attribute vec4 vPosition;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	gl_Position = u_modelViewProjMatrix * vPosition;
	v_texCoord = vTexCoord.st;
	vec4 transNormal = u_normalMatrix * vec4(vNormal, 1);
	v_Dot = max(dot(transNormal.xyz, lightDir), 0.0);
  }
</script>

<script id="fshader" type="x-shader/x-fragment">
  #extension OES_EGL_image_external : enable
  precision mediump float;

  uniform samplerExternalOES videoSampler;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	vec2 texCoord = vec2(v_texCoord.s, 1.0 - v_texCoord.t);
	vec4 color = texture2D(videoSampler, texCoord);
	color += vec4(0.1, 0.1, 0.1, 1);
	gl_FragColor = vec4(color.xyz * v_Dot, color.a);
  }
</script>

<script>
  // Use object for "global" variables to avoid polluting the global space.
  var g = {};
  
  //
  // Latency object
  //
  // This object keeps track of latency using an exponential moving average.
  //
  Latency = function(gl, vstream)
  {
	this.gl = gl;
	this.latencyUpdateInterval = 500;

	this.acquireTime = -1;
	this.lastLatency = -1;
	this.emaLatency = -1;
	const periods = 15;
	this.alpha = 2 / (periods + 1);
	self = this;
	var ul = function() { self.updateLatency() }
	setInterval(ul, this.latencyUpdateInterval);
  }
  
  Latency.prototype.updateLatency = function()
  {
	  vstream.setConsumerLatency(this.emaLatency);
  }
  
  Latency.prototype.snapshot = function()
  {
	var latency;
	if (this.acquireTime > 0) {
	  latency = gl.dte.getSyncValues().ust - this.acquireTime;	      
	}
	this.acquireTime = gl.dte.ustnow();
	if (lastLatency > 0) {
	  emaLatency = this.alpha * this.lastLatency + (1 - this.alpha) * emaLatency;
	} else {
	  emaLatency = latency;
	}
	this.lastLatency = latency;
  }

  ///////////////////////////////////////////////////////////////////////
  // Create a video texture and bind a source to it.
  ///////////////////////////////////////////////////////////////////////

  // Array of files currently loading
  g.loadingFiles = [];

  // Clears all the files currently loading.
  // This is used to handle context lost events.
  function clearLoadingFiles() {
    for (var ii = 0; ii < g.loadingFiles.length; ++ii) {
      g.loadingFiles[ii].onload = undefined;
    }
    g.loadingFiles = [];
  }

  // Create a hidden video element and append to the document
  // XXX Is it necessary to append the element?
  function createVideoElement() {
	// Use an 'audio' element as, in this use, no playback area is needed.
	// XXX Will this work or do I need to use 'video' in order that the canvas
	// will become tainted if the video is cross-origin? The extension will have
	// to deal with tainting ...
	var video = document.createElement('audio');
	video.autoplay = false;  // Need to do some housekeeping after loading and before playing.
	video.controls = false;
	video.hidden = true;
	document.body.appendChild(video);
	return video;
  }


  //
  // connectVideo
  //
  // Connect video from the passed HTMLVideoElement to the texture
  // currently bound to TEXTURE_EXTERNAL_OES on the active texture
  // unit.
  //
  // First a wdtStream object is created with its consumer set to
  // the texture. Once the video is loaded, it is set as the
  // producer. This could potentially fail, depending on the
  // video format.
  //
  // interface wdtStream {
  //   enum state {
  //     // Consumer connected; waiting for producer to connect
  //     wdtStreamConnecting,
  //     // Producer & consumer connected. No frames yet. */
  //     wdtStreamEmpty,
  //     wdtStreamNewFrameAvailable,
  //     wdtStreamOldFrameAvailable,
  //     wdtStreamDisconnected
  //   };
  //   // Time taken from acquireImage to posting drawing buffer; default 0?
  //   readonly int consumerLatency; // microseconds
  //   // Frame # (aka Media Stream Count) of most recently inserted frame
  //   // Value is 1 at first frame.
  //   readonly int producerFrame;
  //   // MSC of most recently acquired frame. 
  //   readonly int consumerFrame;
  //   // timeout for acquireImage; default 0
  //   int acquireTimeout; // microseconds
  //
  //   // Sets consumerLatency. Clamp to some maximum set by the
  //   // producer?
  //   void setConsumerLatency(int);
  // };
  //
 
  function loadVideo(video, url) {
	video.onload = function() {
	  g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
	  g.videoReady = true;
	  assert(stream.state == wdtStreamEmpty);
    };
	video.onerror = function() {
	  g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
	  window.alert("Video load failed: " + video.error);
	  // Abort application?
	};
	video.src = url; // Start the load
	g.loadingFiles.push(video);	
  }


  ///////////////////////////////////////////////////////////////////////
  // Initialize the application.
  ///////////////////////////////////////////////////////////////////////

  function init() {
    // Initialize
	var gl;
	var program;
	
	g.videoReady = false;
	
    gl = initWebGL(
        // The id of the Canvas Element
        "videotex");
    if (!gl) {
      return;
    }
    gl.dte = gl.getExtension("WEBGL_dynamic_texture");
	if (!gl.dte) {
	  return;
	}
    program = simpleSetup(
      gl,
      // The ids of the vertex and fragment shaders
      "vshader", "fshader",
      // The vertex attribute names used by the shaders.
      // The order they appear here corresponds to their index
      // used later.
      [ "vNormal", "vColor", "vPosition"],
      // The clear color and depth values
      [ 0.259, 0.388, 1, 1 ], 10000);

	g.videoTexture = {
	  texture: gl.createTexture(),
	  stream: null
	};
    // gl.activeTexture(gl.TEXTURE0);
	gl.bindTexture(gl.TEXTURE_EXTERNAL_OES, g.videoTexture.texture);
	// Create stream with currently bound texture as consumer.
	// XXX Pass producer at creation in case there need to be major differences in
	// stream types. Can be eased after implementation experience.
	// Throws if video already connected to stream or video loading or loaded.
	// Can also be eased after implementation experience.
	g.videoTexture.stream = gl.dte.createStream(); 
	// XXX Maybe do here or else after measuring and setting maxLatency.
	g.videoTexture.stream.connectProducer(g.video);

    // Set some uniform variables for the shaders
    gl.uniform3f(gl.getUniformLocation(program, "lightDir"), 0, 0, 1);
	// video texture is bound to TEXTURE0.
    gl.uniform1i(gl.getUniformLocation(program, "videoSampler"), 0);

    // Create a box. On return 'gl' contains a 'box' property with
    // the BufferObjects containing the arrays for vertices,
    // normals, texture coords, and indices.
    g.box = makeBox(gl);

    // Create some matrices to use later and save their locations in the shaders
    g.mvMatrix = new J3DIMatrix4();
    g.u_normalMatrixLoc = gl.getUniformLocation(program, "u_normalMatrix");
    g.normalMatrix = new J3DIMatrix4();
    g.u_modelViewProjMatrixLoc =
            gl.getUniformLocation(program, "u_modelViewProjMatrix");
    g.mvpMatrix = new J3DIMatrix4();

    // Enable all of the vertex attribute arrays.
    gl.enableVertexAttribArray(0);
    gl.enableVertexAttribArray(1);
    gl.enableVertexAttribArray(2);

    // Set up all the vertex attributes for vertices, normals and texCoords
    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.vertexObject);
    gl.vertexAttribPointer(2, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.normalObject);
    gl.vertexAttribPointer(0, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.texCoordObject);
    gl.vertexAttribPointer(1, 2, gl.FLOAT, false, 0, 0);

    // Bind the index array
    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, g.box.indexObject);

    return gl;
  }

  function reshape(gl) {
	  // change the size of the canvas's backing store to match the size it is displayed.
	  var canvas = document.getElementById('example');
	  var devicePixelRatio = window.devicePixelRatio || 1;
	  var clientWidthDP = canvas.clientWidth * devicePixelRatio;
	  var clientHeightDP = canvas.clientHeight * devicePixelRatio;
	  if (clientWidthDP == canvas.width && clientHeightDP == canvas.height)
		  return;

	  canvas.width = clientWidthDP;
	  canvas.height = clientHeightDP;

	  // Set the viewport and projection matrix for the scene
	  gl.viewport(0, 0, clientWidthDP, clientHeightDP);
	  g.perspectiveMatrix = new J3DIMatrix4();
	  g.perspectiveMatrix.perspective(30, clientWidthDP / clientHeightDP, 1, 10000);
	  g.perspectiveMatrix.lookat(0, 0, 7, 0, 0, 0, 0, 1, 0);
  }

  function drawFrame(gl) {
    var lastFrame;
	var syncValues;
	var latency;
	var graphicsMSCBase;
	var presentationTime;

    // Make sure the canvas is sized correctly.
    reshape(gl);

    // Clear the canvas
    gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

    // Make a model/view matrix.
    g.mvMatrix.makeIdentity();
    g.mvMatrix.rotate(20, 1,0,0);
    g.mvMatrix.rotate(currentAngle, 0,1,0);

    // Construct the normal matrix from the model-view matrix and pass it in
    g.normalMatrix.load(g.mvMatrix);
    g.normalMatrix.invert();
    g.normalMatrix.transpose();
    g.normalMatrix.setUniform(gl, g.u_normalMatrixLoc, false);

    // Construct the model-view * projection matrix and pass it in
    g.mvpMatrix.load(g.perspectiveMatrix);
    g.mvpMatrix.multiply(g.mvMatrix);
    g.mvpMatrix.setUniform(gl, g.u_modelViewProjMatrixLoc, false);

	// To avoid duplicating everything below for each option, use a
	// temporary variable. This will not be necessary in the final
	// code.
	// OPTION 1: augmented video object
	var vstream = g.video.wdtStream;
	// OPTION 2: separate stream object
	var vstream = g.vstream;

	// In the following
	//   UST is a monotonically increasing counter never adjusted by NTP etc.
	//   The unit is nanoseconds but the frequency of update will vary from
	//   system to system. The average frequency at which the counter is
	//   updated should be 5x the highest MSC frequency supported. For
	//   example if highest MSC is 48kHz (audio) the update frequency
	//   should be 240kHz. Most OSes have this kind of counter available.
	//
	//   If OS provided UST=0 would be system startup. Could also use
	//   {navigation,fetch}Start as in the high-resolution time spec.
	//
	//   MSC is the media stream count. It is incremented once/sample; for
	//   video that means once/frame, for audio once/sample. For graphics,
	//   it is incremented once/screen refresh.
	//
	//   CPC is the canvas presentation count. It is incremented once
	//   each time the canvas is presented.
	//
	
	if (graphicsMSCBase == undefined) {
	    graphicsMSCBase = gl.dte.getSyncValues().msc;
	}
	/*
    if (lastFrame.msc && vstream.producerFrame > lastFrame.msc + 1) {
      // Missed a frame! Simplify rendering?
	}
	*/
	if (!latency.frameCount) {
	  // Initialize
	  latency.frameCount = 0;
	  latency.accumTotal = 0;
	}

	if (lastFrame) {
	  syncValues = gl.dte.getSyncValues();
	  // interface syncValues {
	  //     // UST of last present
	  //     readonly attribute long long ust;
	  //     // Screen refresh count (aka MSC) at last present
	  //     // Initialized to 0 on browser start
	  //     readonly attribute long msc;
	  //     // Canvas presentation count at last present
	  //     // Initialized to 0 at canvas creation.
	  //     readonly attribute long cpc;
	  // };
	  // XXX What happens to cpc when switch to another tab?
	  if (syncValues.msc - graphicsMSCBase != syncValues.cpc) {
	    // We are not keeping up with screen refresh!
		// Or are we? If cpc increment stops when canvas hidden,
		// will need some way to know canvas was hidden so app
		// won't just assume its not keeping up and therefore
		// adjust its rendering.
	    graphicsMSCBase = syncValues.msc - syncValues.cpc; // reset base.
	  }
	  // Have to account for frames showing for a long time
	  // (i.e. no new frames) so lastFrame.ust might be long in the past.
	  latency.accumValue += syncValues.ust - lastFrame.ust;
	  latency.frameCount++;
	  if (latency.frameCount == 30) {
	    vstream.setConsumerLatency(latency.accumValue / 30);
		latency.frameCount = 0;
	    latency.accumValue = 0;	
	  }
	}  

	if (g.videoReady) {
	  if (g.videoTexture.stream.acquireImage()) {
		g.latency.snapshot();
		// Record UST of frame acquisition.
		// No such system function in JS so it is added to extension.
		lastFrame.ust = gl.dte.ustnow();
		// Or possibly
		lastFrame.ust = window.performance.now();
		// but there are issues.
		// - it doesn't meet the requirements listed above but is it sufficient?
		//   - the units are milliseconds but microsecond accuracy is
		//     (nominally) required. Presumably it is supposed to report
		//     fractional values.
		// The above timer comes from the high-resolution time proposal
		//
		//lastFrame.msc = vstream.consumerFrame;
		// Record requested presentation time of frame
		lastFrame.pt = vstream.consumerFrame.pt;
		// Set presentation time on the drawbuffer.
		gl.dte.setDBPresentationTime(g.videoTexture.stream.consumerFrame.pt);
	    // If have explicit present pass above value as parameter to extended present function.
	    // If have separate drawbuffer object, set directly on drawbuffer.
	  }
	}

    // Draw the cube
    gl.drawElements(gl.TRIANGLES, g.box.numIndices, gl.UNSIGNED_BYTE, 0);

    if (g.videoReady)
      vtream.releaseImage();

    // Show the framerate
    framerate.snapshot();

    currentAngle += incAngle;
    if (currentAngle > 360)
        currentAngle -= 360;	
  }

  function handleContextLost(e) {
	e.preventDefault();
	clearLoadingFiles();
	if (g.requestId !== undefined) {
	  window.cancelAnimFrame(g.requestId);
	  g.requestId = undefined;
	}
	if (g.videoReady) {
	  g.videoTexture.video.pause();
	}
  }
  
  function handleContextRestored() {
    start();
  }
  
  function measurePerf() {
	if (g.frameTimer.emaTime > minInterFrameInterval) {
		// choose simpler rendering
        if (drawFunc > 1)
		  drawFunc--;
	}
	window.requestAnimationFrame(
  }


  function start() {
	var nframes = 0;
	var minInterframeInterval = -1;
	
    var gl = init();
    if (!gl) {
     return;
    }
	
	g.latencytimer = new Latency(gl, g.videoTexture.stream);
	g.frametimer = new Latency();
	
	var f1 = function() {
	  drawFrameComplex();
      g.requestId = window.requestAnimFrame(f1, c);
	}
	
	var f2 = function() {
	  drawFrameMedium();
      g.requestId = window.requestAnimFrame(f2, c);
	}
	
	var f3 = function() {
	  drawFrameSimple();
      g.requestId = window.requestAnimFrame(f3, c);
	}
	
    var f = function() {
      drawFrameComplex(gl);
	  if (g.videoReady) {
		  minInterframeInterval = g.videoTexture.stream.minInterframeInterval;
	  }
	  
	  if (minInterframeInterval && nframes > 15) {
		if (g.frameTimer.emaTime > minInterFrameInterval) {
			// choose simpler rendering
		}
		// XXX Can only be called when state is empty or connecting? But what about displaying the poster or the frame
		// when the video is paused after a context lost? Can max latency be set as long as the video is paused? Seems
		// it might be necessary to do this.
		g.videoTexture.stream.setMaxLatency(g.frameTimer.emaTime);
		// This could be a big hiccough. Maybe better to connect during set up and have it enter some special state
		// that still allows maxLatency to be set.
		g.videoTexture.stream.connectProducer(g.video);
		// Start the video playing.
		g.videoTexture.video.play();
        g.requestId = window.requestAnimFrame(f, c);
	  } else {
        g.requestId = window.requestAnimFrame(f1, c);
	  }
    };
    f();
  }
  
  function main() {
    var c = document.getElementById("example");

    //c = WebGLDebugUtils.makeLostContextSimulatingCanvas(c);
    // tell the simulator when to lose context.
    //c.loseContextInNCalls(20);

    c.addEventListener('webglcontextlost', handleContextLost, false);
    c.addEventListener('webglcontextrestored', handleContextRestored, false);

    currentAngle = 0;
    incAngle = 0.5;
    framerate = new Framerate("framerate");

    // Set-up video here; don't want it recreated and reloaded on webglcontextrestored.	
	g.video = createVideoElement();
	loadVideo(g.video, "http://myserver/myvideo.mp4");
	
	start();
  }


</script>
</head>

<body onLoad="start()">
<canvas id="example">
  If you're seeing this your web browser doesn't support the &lt;canvas>&gt; element. Ouch!
</canvas>
<div id="framerate"></div>
</body>

</html>
