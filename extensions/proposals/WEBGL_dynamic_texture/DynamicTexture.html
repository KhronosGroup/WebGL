<!DOCTYPE html>
<!--
/*
 * WEBGL_dynamic_texture sample
 *
 * Copyright (C) 2012 HI Corporation. All Rights Reserved.
 *
 * Based on
 *   https://cvs.khronos.org/svn/repos/registry/trunk/public/webgl/sdk/demos/webkit/SpiritBox.html
 * which carries the following license which propagates to this sample:
 * 
 * Copyright (C) 2009 Apple Inc. All Rights Reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND HI CORPORATION ``AS IS''
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
 * OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
 -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>WEBGL_dynamic_texture Sample</title>
<style>
body, html {
  margin: 0px;
  width: 100%;
  height: 100%;
}
#framerate {
  position: absolute;
  top: 10px;
  left: 10px;
  background-color: rgba(0,0,0,0.3);
  padding: 1em;
  color: white;
}
#example {
  width: 100%;
  height: 100%;
}
canvas {
  display: block;
  border: thick ridge MediumSlateBlue;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
</style>

<!-- For {request,cancel}AnimFrame -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/common/webgl-utils.js"></script>
<!-- For lost context simulation -->
<script src="https://www.khronos.org/registry/webgl/sdk/debug/webgl-debug.js"></script>
<!-- For simpleSetup & makeBox -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DI.js"> </script>
<!-- For matrix math -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DIMath.js"> </script>

<script id="vshader" type="x-shader/x-vertex">
  uniform mat4 u_modelViewProjMatrix;
  uniform mat4 u_normalMatrix;
  uniform vec3 lightDir;

  attribute vec3 vNormal;
  attribute vec4 vTexCoord;
  attribute vec4 vPosition;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	gl_Position = u_modelViewProjMatrix * vPosition;
	v_texCoord = vTexCoord.st;
	vec4 transNormal = u_normalMatrix * vec4(vNormal, 1);
	v_Dot = max(dot(transNormal.xyz, lightDir), 0.0);
  }
</script>

<script id="fshader" type="x-shader/x-fragment">
  #extension OES_EGL_image_external : enable
  precision mediump float;

  uniform samplerExternalOES videoSampler;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	vec2 texCoord = vec2(v_texCoord.s, 1.0 - v_texCoord.t);
	vec4 color = texture2D(videoSampler, texCoord);
	color += vec4(0.1, 0.1, 0.1, 1);
	gl_FragColor = vec4(color.xyz * v_Dot, color.a);
  }
</script>

<script>
  // Use object for "global" variables to avoid polluting the global space.
  var g = {};

  ///////////////////////////////////////////////////////////////////////
  // Create a video texture and bind a source to it.
  ///////////////////////////////////////////////////////////////////////

  // Array of files currently loading
  g.loadingFiles = [];

  // Clears all the files currently loading.
  // This is used to handle context lost events.
  function clearLoadingFiles() {
    for (var ii = 0; ii < g.loadingFiles.length; ++ii) {
      g.loadingFiles[ii].onload = undefined;
    }
    g.loadingFiles = [];
  }

  //
  // connectVideo
  //
  // Connect video from the passed HTMLVideoElement to the texture
  // currently bound to TEXTURE_EXTERNAL_OES on the active texture
  // unit.
  //
  // First a wdtStream object is created with its consumer set to
  // the texture. Once the video is loaded, it is set as the
  // producer. This could potentially fail, depending on the
  // video format.
  //
  // interface wdtStream {
  //   enum state {
  //     // Consumer connected; waiting for producer to connect
  //     wdtStreamConnecting,
  //     // Producer & consumer connected. No frames yet. */
  //     wdtStreamEmpty,
  //     wdtStreamNewFrameAvailable,
  //     wdtStreamOldFrameAvailable,
  //     wdtStreamDisconnected
  //   };
  //   // Time taken from acquireImage to posting drawing buffer; default 0?
  //   readonly int consumerLatency;
  //   // Frame # (aka Media Stream Count) of most recently inserted frame
  //   // Value is 1 at first frame.
  //   readonly int producerFrame;
  //   // MSC of most recently acquired frame. 
  //   readonly int consumerFrame;
  //   // timeout for acquireImage; default 0
  //   int acquireTimeout;
  //
  //   void setConsumerLatency(int);
  // };
  //
 
  //
  function connectVideo(ctx, video)
  {
    g.loadingFiles.push(video);
	g.videoReady = false;

	//-----------------------------
	// Options for connecting to video
	//-----------------------------
	// OPTION 1: method on WDT extension augments video element
	// with a wdtStream object.
	ctx.dte.createStream(video);
	assert(video.wdtStream.state == wdtStreamConnecting);
    //-----------------------------
	// OPTION 2: method returns a stream object.
	g.vstream = ctx.dte.createStream(ctx);
	assert(g.vstream.state == wdtStreamConnecting);
    //-----------------------------

    video.onload = function() {
        g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
        try {
          // OPTION 1: video object augmented with stream
          video.wdtStream.connect();
	      assert(video.wdtStream.state == wdtStreamEmpty);
          //-----------------------------
          // OPTION 2: separate stream object
          g.vstream.connectProducer(video); 
	      assert(g.stream.state == wdtStreamEmpty);
		  //------------------------------
		  if (!video.autoplay) {
		    video.play(); // Play video
		  }
		  g.videoReady = true;
        } catch (e) {
          window.alert("Video texture setup failed: " + e.name);
        }
      };
  }

  ///////////////////////////////////////////////////////////////////////
  // Initialize the application.
  ///////////////////////////////////////////////////////////////////////

  function init()
  {
    // Initialize
    var gl = initWebGL(
        // The id of the Canvas Element
        "example");
    if (!gl) {
      return;
    }
    gl.dte = gl.getExtension("WEBGL_dynamic_texture");
    var program = simpleSetup(
      gl,
      // The ids of the vertex and fragment shaders
      "vshader", "fshader",
      // The vertex attribute names used by the shaders.
      // The order they appear here corresponds to their index
      // used later.
      [ "vNormal", "vColor", "vPosition"],
      // The clear color and depth values
      [ 0.259, 0.388, 1, 1 ], 10000);

	// Set up a video texture
    g.video = document.getElementById(videoId);
	var texture = ctx.createTexture();
    // ctx.activeTexture(gl.TEXTURE0);
    ctx.bindTexture(ctx.TEXTURE_EXTERNAL_OES, texture);
    connectVideo(gl, g.video);

    // Set some uniform variables for the shaders
    gl.uniform3f(gl.getUniformLocation(program, "lightDir"), 0, 0, 1);
	// video texture is bound to TEXTURE0.
    gl.uniform1i(gl.getUniformLocation(program, "videoSampler"), 0);

    // Create a box. On return 'gl' contains a 'box' property with
    // the BufferObjects containing the arrays for vertices,
    // normals, texture coords, and indices.
    g.box = makeBox(gl);

    // Create some matrices to use later and save their locations in the shaders
    g.mvMatrix = new J3DIMatrix4();
    g.u_normalMatrixLoc = gl.getUniformLocation(program, "u_normalMatrix");
    g.normalMatrix = new J3DIMatrix4();
    g.u_modelViewProjMatrixLoc =
            gl.getUniformLocation(program, "u_modelViewProjMatrix");
    g.mvpMatrix = new J3DIMatrix4();

    // Enable all of the vertex attribute arrays.
    gl.enableVertexAttribArray(0);
    gl.enableVertexAttribArray(1);
    gl.enableVertexAttribArray(2);

    // Set up all the vertex attributes for vertices, normals and texCoords
    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.vertexObject);
    gl.vertexAttribPointer(2, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.normalObject);
    gl.vertexAttribPointer(0, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.texCoordObject);
    gl.vertexAttribPointer(1, 2, gl.FLOAT, false, 0, 0);

    // Bind the index array
    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, g.box.indexObject);

    return gl;
  }

  function reshape(gl)
  {
	  // change the size of the canvas's backing store to match the size it is displayed.
	  var canvas = document.getElementById('example');
	  var devicePixelRatio = window.devicePixelRatio || 1;
	  var clientWidthDP = canvas.clientWidth * devicePixelRatio;
	  var clientHeightDP = canvas.clientHeight * devicePixelRatio;
	  if (clientWidthDP == canvas.width && clientHeightDP == canvas.height)
		  return;

	  canvas.width = clientWidthDP;
	  canvas.height = clientHeightDP;

	  // Set the viewport and projection matrix for the scene
	  gl.viewport(0, 0, clientWidthDP, clientHeightDP);
	  g.perspectiveMatrix = new J3DIMatrix4();
	  g.perspectiveMatrix.perspective(30, clientWidthDP / clientHeightDP, 1, 10000);
	  g.perspectiveMatrix.lookat(0, 0, 7, 0, 0, 0, 0, 1, 0);
  }

  function drawFrame(gl)
  {
    var lastFrame;
	var syncValues;
	var latency;
	var graphicsMSCBase;

    // Make sure the canvas is sized correctly.
    reshape(gl);

    // Clear the canvas
    gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

    // Make a model/view matrix.
    g.mvMatrix.makeIdentity();
    g.mvMatrix.rotate(20, 1,0,0);
    g.mvMatrix.rotate(currentAngle, 0,1,0);

    // Construct the normal matrix from the model-view matrix and pass it in
    g.normalMatrix.load(g.mvMatrix);
    g.normalMatrix.invert();
    g.normalMatrix.transpose();
    g.normalMatrix.setUniform(gl, g.u_normalMatrixLoc, false);

    // Construct the model-view * projection matrix and pass it in
    g.mvpMatrix.load(g.perspectiveMatrix);
    g.mvpMatrix.multiply(g.mvMatrix);
    g.mvpMatrix.setUniform(gl, g.u_modelViewProjMatrixLoc, false);

	// To avoid duplicating everything below for each option, use a
	// temporary variable. This will not be necessary in the final
	// code.
	// OPTION 1: augmented video object
	var vstream = g.video.wdtStream;
	// OPTION 2: separate stream object
	var vstream = g.vstream;

	// In the following
	//   UST is a monotonically increasing counter never adjusted by NTP etc.
	//   The unit is nanoseconds but the frequency of update will vary from
	//   system to system. The average frequency at which the counter is
	//   updated should be 5x the highest MSC frequency supported. For
	//   example if highest MSC is 48kHz (audio) the update frequency
	//   should be 240kHz. Most OSes have this kind of counter available.
	//
	//   MSC is the media stream count. It is incremented once/sample; for
	//   video that means once/frame, for audio once/sample. For graphics,
	//   it is incremented once/screen refresh.
	//
	//   CPC is the canvas presentation count. It is incremented once
	//   each time the canvas is presented.
	//
	
	if (graphicsMSCBase == undefined) {
	    graphicsMSCBase = gl.dte.getSyncValues().msc;
	}

    if (lastFrame.msc && vstream.producerFrame > lastFrame.msc + 1) {
      // Missed a frame! Simplify rendering?
	}

	if (!latency.frameCount) {
	  // Initialize
	  latency.frameCount = 0;
	  latency.accumTotal = 0;
	}

	if (lastFrame.ust) {
	  syncValues = gl.dte.getSyncValues();
	  // interface syncValues {
	  //     // UST of last present
	  //     readonly attribute long long ust;
	  //     // Screen refresh count (aka MSC) at last present
	  //     // Initialized to 0 on browser start
	  //     readonly attribute long msc;
	  //     // Canvas presentation count at last present
	  //     // Initialized to 0 at canvas creation.
	  //     readonly attribute long cpc;
	  // };
	  // XXX What happens to cpc when switch to another tab?
	  if (syncValues.msc - graphicsMSCBase != syncValues.cpc) {
	    // We are not keeping up with screen refresh!
		// Or are we? If cpc increment stops when canvas hidden,
		// will need some way to know canvas was hidden so app
		// won't just assume its not keeping up and therefore
		// adjust its rendering.
	    graphicsMSCBase = syncValues.msc; // reset base.
	  }
	  latency.accumValue += syncValues.ust - lastFrame.ust;
	  latency.frameCount++;
	  if (latency.frameCount == 30) {
	    vstream.setConsumerLatency(latency.accumValue / 30);
		latency.frameCount = 0;
	    latency.accumValue = 0;	
	  }
	}  

	if (g.videoReady) {
	  if (g.video.wdtStream.acquireImage()) {
		// Record UST of frame acquisition.
		// No such system function in JS so it is added to extension.
		lastFrame.ust = gl.dte.ustnow();
		lastFrame.msc = vstream.consumerFrame;
	  }
      vstream.acquireImage();
	  lastFrame = g.stream.consumerFrame;
    }

    // Draw the cube
    gl.drawElements(gl.TRIANGLES, g.box.numIndices, gl.UNSIGNED_BYTE, 0);

    if (g.videoReady)
      vtream.releaseImage();

    // Show the framerate
    framerate.snapshot();

    currentAngle += incAngle;
    if (currentAngle > 360)
        currentAngle -= 360;
  }

  function start()
  {
    var c = document.getElementById("example");

    //c = WebGLDebugUtils.makeLostContextSimulatingCanvas(c);
    // tell the simulator when to lose context.
    //c.loseContextInNCalls(20);

    c.addEventListener('webglcontextlost', handleContextLost, false);
    c.addEventListener('webglcontextrestored', handleContextRestored, false);

    var gl = init();
    if (!gl) {
     return;
    }

    currentAngle = 0;
    incAngle = 0.5;
    framerate = new Framerate("framerate");
    var f = function() {
      drawFrame(gl);
      g.requestId = window.requestAnimFrame(f, c);
    };
    f();

    function handleContextLost(e) {
      e.preventDefault();
      clearLoadingFiles();
      if (g.requestId !== undefined) {
        window.cancelAnimFrame(g.requestId);
        g.requestId = undefined;
      }
    }

    function handleContextRestored() {
      init();
      f();
    }
  }
</script>
</head>

<body onload="start()">
<canvas id="example">
  If you're seeing this your web browser doesn't support the &lt;canvas>&gt; element. Ouch!
</canvas>
<div id="framerate"></div>
</body>

</html>
