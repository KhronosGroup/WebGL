<!DOCTYPE html>
<!-- Questions to answer

Q: Do videos with variable framerate have a target display rate?
A: No such thing is exposed in the HTMLVideoElement or MediaElement interfaces.
   The containers usually store the framerate as metadata (in WebM FrameRate
   is listed as "information only."). However browsers may not use that.
   Firefox 4, WebKit and Opera all reportedly use the presentation time
   encoded with the frame.
   See this thread:
   http://lists.whatwg.org/pipermail/whatwg-whatwg.org/2011-January/029801.html
   The thread starts at
   http://lists.whatwg.org/pipermail/whatwg-whatwg.org/2011-January/029724.html
   There is no resolution to the request for a way to determine fps.

   WebM container spec: http://www.webmproject.org/code/specs/container/ 

Q: What do applications need to be able to determine about the source?
A: Two things
   - the minimum interframe interval, which is probably the same as the
     framerate given in the container. This is to determine a rendering
	 budget. Need to ensure methodology works with fixed- and variable-
	 frame-rate videos.
   - whether a frame has been missed. In EGLStream this can be determined
     by comparing the PRODUCER_FRAME_KHR and CONSUMER_FRAME_KHR attributes.

Q: Should we make connection setup a convenience function with an onload
   handler callback?
A: TBD

Q: Use an exponential moving average for calculating latency?
A: TBD.

Q: What should be the interface for connecting a stream to a producer?
A: KHR_stream_producer_aldatalocator has the app pass an XADataSink
   structure with, essentially, a pointer to the EGLStream, as the
   ImageVideoSnk argument when calling CreateMediaPlayer.

   Similarly the stream is connected to an EGLSurface producer by
   eglCreateStreamProducerSurfaceKHR(dpy, config, EGLStreamKHR stream, &attrib_list[0]);

   This suggests the stream needs to be passed when creating the video element.
   But video elements do not have a constructor. 

Q: Is the maximum latency set when the producer is bound?
A: Yes.

Q: Do we specify repeat vs pause for the case where latency drops? If so, how?
A: TBD.
-->
<!--
/*
 * WEBGL_dynamic_texture sample
 *
 * Copyright (C) 2012 HI Corporation. All Rights Reserved.
 *
 * Based on
 *   https://cvs.khronos.org/svn/repos/registry/trunk/public/webgl/sdk/demos/webkit/SpiritBox.html
 * which carries the following license which propagates to this sample:
 * 
 * Copyright (C) 2009 Apple Inc. All Rights Reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND HI CORPORATION ``AS IS''
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
 * OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
 -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>WEBGL_dynamic_texture Sample</title>
<style>
body, html {
  margin: 0px;
  width: 100%;
  height: 100%;
}
#framerate {
  position: absolute;
  top: 10px;
  left: 10px;
  background-color: rgba(0,0,0,0.3);
  padding: 1em;
  color: white;
}
#example {
  width: 100%;
  height: 100%;
}
canvas {
  display: block;
  border: thick ridge MediumSlateBlue;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
</style>

<!-- For {request,cancel}AnimFrame -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/common/webgl-utils.js"></script>
<!-- For lost context simulation -->
<script src="https://www.khronos.org/registry/webgl/sdk/debug/webgl-debug.js"></script>
<!-- For simpleSetup & makeBox -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DI.js"> </script>
<!-- For matrix math -->
<script src="https://www.khronos.org/registry/webgl/sdk/demos/webkit/resources/J3DIMath.js"> </script>

<script id="vshader" type="x-shader/x-vertex">
  uniform mat4 u_modelViewProjMatrix;
  uniform mat4 u_normalMatrix;
  uniform vec3 lightDir;

  attribute vec3 vNormal;
  attribute vec4 vTexCoord;
  attribute vec4 vPosition;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	gl_Position = u_modelViewProjMatrix * vPosition;
	v_texCoord = vTexCoord.st;
	vec4 transNormal = u_normalMatrix * vec4(vNormal, 1);
	v_Dot = max(dot(transNormal.xyz, lightDir), 0.0);
  }
</script>

<script id="fshader" type="x-shader/x-fragment">
  #extension OES_EGL_image_external : enable
  precision mediump float;

  uniform samplerExternalOES videoSampler;

  varying float v_Dot;
  varying vec2 v_texCoord;

  void main()
  {
	vec2 texCoord = vec2(v_texCoord.s, 1.0 - v_texCoord.t);
	vec4 color = texture2D(videoSampler, texCoord);
	color += vec4(0.1, 0.1, 0.1, 1);
	gl_FragColor = vec4(color.xyz * v_Dot, color.a);
  }
</script>

<script>
  // Use object for "global" variables to avoid polluting the global space.
  var g = {};

  ///////////////////////////////////////////////////////////////////////
  // Create a video texture and bind a source to it.
  ///////////////////////////////////////////////////////////////////////

  // Array of files currently loading
  g.loadingFiles = [];

  // Clears all the files currently loading.
  // This is used to handle context lost events.
  function clearLoadingFiles() {
    for (var ii = 0; ii < g.loadingFiles.length; ++ii) {
      g.loadingFiles[ii].onload = undefined;
	  g.loadingFiles[ii].wssDisconnectStream();
    }
    g.loadingFiles = [];
  }

  // Create a hidden video element and append to the document
  // XXX Is it necessary to append the element?
  function createVideoElement() {
	var video = document.createElement('video');
	video.autoplay = true;
	video.controls = false;
	video.hidden = true;
	document.body.appendChild(video);
	return video;
  }


  //
  // connectVideo
  //
  // Connect video from the passed HTMLVideoElement to the texture
  // currently bound to TEXTURE_EXTERNAL_OES on the active texture
  // unit.
  //
  // First a wdtStream object is created with its consumer set to
  // the texture. Once the video is loaded, it is set as the
  // producer. This could potentially fail, depending on the
  // video format.
  //
  // interface wdtStream {
  //   enum state {
  //     // Consumer connected; waiting for producer to connect
  //     wdtStreamConnecting,
  //     // Producer & consumer connected. No frames yet. */
  //     wdtStreamEmpty,
  //     wdtStreamNewFrameAvailable,
  //     wdtStreamOldFrameAvailable,
  //     wdtStreamDisconnected
  //   };
  //   // Time taken from acquireImage to posting drawing buffer; default 0?
  //   readonly int consumerLatency; // microseconds
  //   // Frame # (aka Media Stream Count) of most recently inserted frame
  //   // Value is 1 at first frame.
  //   readonly int producerFrame;
  //   // MSC of most recently acquired frame. 
  //   readonly int consumerFrame;
  //   // timeout for acquireImage; default 0
  //   int acquireTimeout; // microseconds
  //
  //   // Sets consumerLatency. Clamp to some maximum set by the
  //   // producer?
  //   void setConsumerLatency(int);
  // };
  //
 
  function connectStreamToProducerAndSource(stream, video, url) {
	video.onload = function() {
	  g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
    };
	video.onerror = function() {
	  g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
	  // Notify user? Abort application?
	  // video.error gives the reason.
	};
    g.loadingFiles.push(video);
	video.wssConnectToSourceAsProducer(url, stream);
  }

  function connectVideo(ctx, video) {
	// Is this whole thing racy? Is the video already loading
	// when this is called?
    g.loadingFiles.push(video);
	g.videoReady = false;

	//-----------------------------
	// Options for connecting to video
	//-----------------------------
	// OPTION 1: method on WDT extension augments video element
	// with a wdtStream object.
	ctx.dte.createStream(video);
	assert(video.wdtStream.state == wdtStreamConnecting);
    //-----------------------------
	// OPTION 2: method returns a stream object.
	g.vstream = ctx.dte.createStream(ctx);
	assert(g.vstream.state == wdtStreamConnecting);
    //-----------------------------

    video.onload = function() {
        g.loadingFiles.splice(g.loadingFiles.indexOf(video), 1);
        try {
          // OPTION 1: video object augmented with stream
          video.wdtStream.connect();
	      assert(video.wdtStream.state == wdtStreamEmpty);
          //-----------------------------
          // OPTION 2: separate stream object
          g.vstream.connectProducer(video); 
	      assert(g.stream.state == wdtStreamEmpty);
		  //------------------------------
		  if (!video.autoplay) {
		    video.play(); // Play video
		  }
		  g.videoReady = true;
        } catch (e) {
          window.alert("Video texture setup failed: " + e.name);
        }
      };
  }

  ///////////////////////////////////////////////////////////////////////
  // Initialize the application.
  ///////////////////////////////////////////////////////////////////////

  function init() {
    // Initialize
    var gl = initWebGL(
        // The id of the Canvas Element
        "example");
    if (!gl) {
      return;
    }
    gl.dte = gl.getExtension("WEBGL_dynamic_texture");
    var program = simpleSetup(
      gl,
      // The ids of the vertex and fragment shaders
      "vshader", "fshader",
      // The vertex attribute names used by the shaders.
      // The order they appear here corresponds to their index
      // used later.
      [ "vNormal", "vColor", "vPosition"],
      // The clear color and depth values
      [ 0.259, 0.388, 1, 1 ], 10000);

	g.videoTexture = {
	  texture: gl.createTexture(),
	  video: createVideoElement(),
	  stream: null
	};
    // gl.activeTexture(gl.TEXTURE0);
	gl.bindTexture(gl.TEXTURE_EXTERNAL_OES, g.videoTexture.texture);
	// Create stream with currently bound texture as consumer.
	g.videoTexture.stream = gl.dte.createStream();

	connectStreamToProducerAndSource(stream, video, "http://myserver/myvideo.mp4");
//-------------
	// Option 1: video object is augmented with stream
	connectVideoStream(gl, g.videoTexture.video, "http://myserver/myvideo.mp4");
	// Option 2: returns stream object
	g.videoTexture.vstream = connectVideoStream(gl, "http://myserver/myvideo.mp4");
	connectVideoStream(gl, g.videoTexture, "http://myserver/myvideo.mp4"); //??
//-------------

    // Set some uniform variables for the shaders
    gl.uniform3f(gl.getUniformLocation(program, "lightDir"), 0, 0, 1);
	// video texture is bound to TEXTURE0.
    gl.uniform1i(gl.getUniformLocation(program, "videoSampler"), 0);

    // Create a box. On return 'gl' contains a 'box' property with
    // the BufferObjects containing the arrays for vertices,
    // normals, texture coords, and indices.
    g.box = makeBox(gl);

    // Create some matrices to use later and save their locations in the shaders
    g.mvMatrix = new J3DIMatrix4();
    g.u_normalMatrixLoc = gl.getUniformLocation(program, "u_normalMatrix");
    g.normalMatrix = new J3DIMatrix4();
    g.u_modelViewProjMatrixLoc =
            gl.getUniformLocation(program, "u_modelViewProjMatrix");
    g.mvpMatrix = new J3DIMatrix4();

    // Enable all of the vertex attribute arrays.
    gl.enableVertexAttribArray(0);
    gl.enableVertexAttribArray(1);
    gl.enableVertexAttribArray(2);

    // Set up all the vertex attributes for vertices, normals and texCoords
    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.vertexObject);
    gl.vertexAttribPointer(2, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.normalObject);
    gl.vertexAttribPointer(0, 3, gl.FLOAT, false, 0, 0);

    gl.bindBuffer(gl.ARRAY_BUFFER, g.box.texCoordObject);
    gl.vertexAttribPointer(1, 2, gl.FLOAT, false, 0, 0);

    // Bind the index array
    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, g.box.indexObject);

    return gl;
  }

  function reshape(gl) {
	  // change the size of the canvas's backing store to match the size it is displayed.
	  var canvas = document.getElementById('example');
	  var devicePixelRatio = window.devicePixelRatio || 1;
	  var clientWidthDP = canvas.clientWidth * devicePixelRatio;
	  var clientHeightDP = canvas.clientHeight * devicePixelRatio;
	  if (clientWidthDP == canvas.width && clientHeightDP == canvas.height)
		  return;

	  canvas.width = clientWidthDP;
	  canvas.height = clientHeightDP;

	  // Set the viewport and projection matrix for the scene
	  gl.viewport(0, 0, clientWidthDP, clientHeightDP);
	  g.perspectiveMatrix = new J3DIMatrix4();
	  g.perspectiveMatrix.perspective(30, clientWidthDP / clientHeightDP, 1, 10000);
	  g.perspectiveMatrix.lookat(0, 0, 7, 0, 0, 0, 0, 1, 0);
  }

  function drawFrame(gl) {
    var lastFrame;
	var syncValues;
	var latency;
	var graphicsMSCBase;

    // Make sure the canvas is sized correctly.
    reshape(gl);

    // Clear the canvas
    gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

    // Make a model/view matrix.
    g.mvMatrix.makeIdentity();
    g.mvMatrix.rotate(20, 1,0,0);
    g.mvMatrix.rotate(currentAngle, 0,1,0);

    // Construct the normal matrix from the model-view matrix and pass it in
    g.normalMatrix.load(g.mvMatrix);
    g.normalMatrix.invert();
    g.normalMatrix.transpose();
    g.normalMatrix.setUniform(gl, g.u_normalMatrixLoc, false);

    // Construct the model-view * projection matrix and pass it in
    g.mvpMatrix.load(g.perspectiveMatrix);
    g.mvpMatrix.multiply(g.mvMatrix);
    g.mvpMatrix.setUniform(gl, g.u_modelViewProjMatrixLoc, false);

	// To avoid duplicating everything below for each option, use a
	// temporary variable. This will not be necessary in the final
	// code.
	// OPTION 1: augmented video object
	var vstream = g.video.wdtStream;
	// OPTION 2: separate stream object
	var vstream = g.vstream;

	// In the following
	//   UST is a monotonically increasing counter never adjusted by NTP etc.
	//   The unit is nanoseconds but the frequency of update will vary from
	//   system to system. The average frequency at which the counter is
	//   updated should be 5x the highest MSC frequency supported. For
	//   example if highest MSC is 48kHz (audio) the update frequency
	//   should be 240kHz. Most OSes have this kind of counter available.
	//
	//   If OS provided UST=0 would be system startup. Could also use
	//   {navigation,fetch}Start as in the high-resolution time spec.
	//
	//   MSC is the media stream count. It is incremented once/sample; for
	//   video that means once/frame, for audio once/sample. For graphics,
	//   it is incremented once/screen refresh.
	//
	//   CPC is the canvas presentation count. It is incremented once
	//   each time the canvas is presented.
	//
	
	if (graphicsMSCBase == undefined) {
	    graphicsMSCBase = gl.dte.getSyncValues().msc;
	}

    if (lastFrame.msc && vstream.producerFrame > lastFrame.msc + 1) {
      // Missed a frame! Simplify rendering?
	}

	if (!latency.frameCount) {
	  // Initialize
	  latency.frameCount = 0;
	  latency.accumTotal = 0;
	}

	if (lastFrame.ust) {
	  syncValues = gl.dte.getSyncValues();
	  // interface syncValues {
	  //     // UST of last present
	  //     readonly attribute long long ust;
	  //     // Screen refresh count (aka MSC) at last present
	  //     // Initialized to 0 on browser start
	  //     readonly attribute long msc;
	  //     // Canvas presentation count at last present
	  //     // Initialized to 0 at canvas creation.
	  //     readonly attribute long cpc;
	  // };
	  // XXX What happens to cpc when switch to another tab?
	  if (syncValues.msc - graphicsMSCBase != syncValues.cpc) {
	    // We are not keeping up with screen refresh!
		// Or are we? If cpc increment stops when canvas hidden,
		// will need some way to know canvas was hidden so app
		// won't just assume its not keeping up and therefore
		// adjust its rendering.
	    graphicsMSCBase = syncValues.msc; // reset base.
	  }
	  latency.accumValue += syncValues.ust - lastFrame.ust;
	  latency.frameCount++;
	  if (latency.frameCount == 30) {
	    vstream.setConsumerLatency(latency.accumValue / 30);
		latency.frameCount = 0;
	    latency.accumValue = 0;	
	  }
	}  

	if (g.videoReady) {
	  if (g.video.wdtStream.acquireImage()) {
		// Record UST of frame acquisition.
		// No such system function in JS so it is added to extension.
		lastFrame.ust = gl.dte.ustnow();
		// Or possibly
		lastFrame.ust = window.performance.now();
		// but there are issues.
		// - it doesn't meet the requirements listed above but is it sufficient?
		//   - the units are milliseconds but microsecond accuracy is
		//     (nominally) required. Presumably it is supposed to report
		//     fractional values.
		// The above timer comes from the high-resolution time proposal
		//
		lastFrame.msc = vstream.consumerFrame;
	  }
      vstream.acquireImage();
	  lastFrame.msc = g.stream.consumerFrame;
    }

    // Draw the cube
    gl.drawElements(gl.TRIANGLES, g.box.numIndices, gl.UNSIGNED_BYTE, 0);

    if (g.videoReady)
      vtream.releaseImage();

    // Show the framerate
    framerate.snapshot();

    currentAngle += incAngle;
    if (currentAngle > 360)
        currentAngle -= 360;
  }

  function start() {
    var c = document.getElementById("example");

    //c = WebGLDebugUtils.makeLostContextSimulatingCanvas(c);
    // tell the simulator when to lose context.
    //c.loseContextInNCalls(20);

    c.addEventListener('webglcontextlost', handleContextLost, false);
    c.addEventListener('webglcontextrestored', handleContextRestored, false);

    var gl = init();
    if (!gl) {
     return;
    }

    currentAngle = 0;
    incAngle = 0.5;
    framerate = new Framerate("framerate");
    var f = function() {
      drawFrame(gl);
      g.requestId = window.requestAnimFrame(f, c);
    };
    f();

    function handleContextLost(e) {
      e.preventDefault();
      clearLoadingFiles();
      if (g.requestId !== undefined) {
        window.cancelAnimFrame(g.requestId);
        g.requestId = undefined;
      }
    }

    function handleContextRestored() {
      init();
      f();
    }
  }
</script>
</head>

<body onload="start()">
<canvas id="example">
  If you're seeing this your web browser doesn't support the &lt;canvas>&gt; element. Ouch!
</canvas>
<div id="framerate"></div>
</body>

</html>
